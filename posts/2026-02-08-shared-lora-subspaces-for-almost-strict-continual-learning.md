# 공유 LoRA 서브스페이스로 지속학습

**부제:** Shared LoRA Subspaces for almost Strict Continual Learning

## 한 줄 결론
단일 공유 LoRA 서브스페이스를 동적으로 업데이트해 파라미터·메모리 사용량을 크게 줄이면서도 지속학습 성능을 유지한다.

## TL;DR (요약)
대규모 사전 학습 모델을 새로운 작업에 효율적으로 지속 적응시키는 것은 재학습 비용과 망각 문제로 어렵다.  
기존 LoRA는 파라미터 효율적이지만 엄격한 지속학습과 지식 통합 메커니즘이 부재하다.  
Share는 단일, 공유된 저차원 서브스페이스를 학습·업데이트해 과거 지식 유지 및 새로운 작업 지식 통합을 동시에 수행한다.  
이를 통해 최대 100배 파라미터 절감, 281배 메모리 절감을 달성하며, 별도 어댑터 없이 다중 태스크 지속학습을 지원한다.

## 문제 정의(Problem)
대규모 사전학습 모델을 다양한 작업에 연속적으로 적응시키는 과정에서:
- 기존 모델의 지식 손실(망각, catastrophic forgetting) 발생  
- 전체 파라미터 재학습 비용 및 메모리 부담  
- 파라미터 효율적 튜닝 기법(LoRA 등)은 지속학습을 위한 지식 통합·전달 메커니즘 미비  
이러한 제약으로 인해 추가 데이터 저장(데이터 리플레이)이나 다수 어댑터 관리가 필요하다.

## 제안 방법(Method)
Share는 하나의 공유 LoRA 저차원 행렬(서브스페이스)을 학습하고, 새 작업에 따라 해당 서브스페이스를 동적으로 확장 및 갱신한다. 방법은 다음과 같다:
1. 기반 서브스페이스 구축: 초기 작업에서 핵심 지식을 추출해 저차원 서브스페이스를 형성  
2. 신규 작업 통합: 새로운 작업에 중요한 서브스페이스 방향(axis)을 식별, 기존 서브스페이스에 증분적으로 병합  
3. 파라미터 업데이트: 단일 서브스페이스 내에서만 파라미터를 조정해 전체 파라미터 수를 크게 줄이고, 과거 작업에 대한 간섭을 최소화  
이 과정은 데이터 리플레이나 다중 어댑터 없이도 태스크 간 지식 전이를 용이하게 한다.

## 핵심 기여/차별점(Contributions)
- 하나의 공유 LoRA 저차원 서브스페이스를 학습·동적 업데이트해 여러 작업과 모달리티를 위한 엄격한 지속학습(strict continual learning)을 구현  
- 최대 100배 파라미터 절감, 281배 메모리 절감을 통해 전통적인 LoRA 대비 자원 효율성을 크게 향상  
- 이미지 분류, 자연어 이해, 3D 포즈 추정, 텍스트-이미지 생성 등 다양한 도메인에서 단일 모델로 과거 지식 유지 및 성능 수준 유지  

## 한계/리스크(Limitations)
- 초록에는 지속학습 과정에서 서브스페이스 확장에 따른 잠재적 연산 복잡도 증가나 확장 한계에 대한 설명이 없다.  
- 신규 도메인이나 극단적 작업 다양성에서 서브스페이스 표현력 한계 여부는 초록 기준으로는 확인 불가.  

## 실무 적용 아이디어(Practical Takeaways)
- 신규 모델 튜닝 시 매 태스크마다 LoRA 어댑터를 생성하는 대신, 공유 서브스페이스를 활용해 파라미터·메모리 오버헤드를 줄일 수 있다.  
- 지속적으로 추가되는 작업 환경에서 기존 서브스페이스에 증분 방향만 업데이트하도록 파이프라인을 구성해 재학습 비용을 최소화한다.  
- 모델 배포 시 하나의 공유 모델로 다양한 태스크를 처리하도록 설계해 어댑터 관리 복잡도를 낮춘다.  

## 메타 정보
- 저자: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille  
- 발행일: 초록 기준으로는 확인 불가  
- 카테고리: Continual Learning, Parameter-Efficient Tuning, Deep Learning  

## 참고 링크
[https://arxiv.org/abs/2602.06043v1](https://arxiv.org/abs/2602.06043v1)
